{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementación de una red neuronal para clasificación con (MLPClassifier) utilizando Sklearn"
      ],
      "metadata": {
        "id": "K0oD8xA1Zoeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UQsJcOC_UjGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec562b64-ae8f-48bc-8928-7c434e541c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.16374923\n",
            "Iteration 2, loss = 1.14090611\n",
            "Iteration 3, loss = 1.11857816\n",
            "Iteration 4, loss = 1.09677375\n",
            "Iteration 5, loss = 1.07547149\n",
            "Iteration 6, loss = 1.05468504\n",
            "Iteration 7, loss = 1.03440573\n",
            "Iteration 8, loss = 1.01462373\n",
            "Iteration 9, loss = 0.99535820\n",
            "Iteration 10, loss = 0.97658926\n",
            "Iteration 11, loss = 0.95833257\n",
            "Iteration 12, loss = 0.94053852\n",
            "Iteration 13, loss = 0.92322169\n",
            "Iteration 14, loss = 0.90638373\n",
            "Iteration 15, loss = 0.89000670\n",
            "Iteration 16, loss = 0.87408395\n",
            "Iteration 17, loss = 0.85861030\n",
            "Iteration 18, loss = 0.84357120\n",
            "Iteration 19, loss = 0.82895431\n",
            "Iteration 20, loss = 0.81476466\n",
            "Iteration 21, loss = 0.80097410\n",
            "Iteration 22, loss = 0.78759157\n",
            "Iteration 23, loss = 0.77460302\n",
            "Iteration 24, loss = 0.76199129\n",
            "Iteration 25, loss = 0.74975377\n",
            "Iteration 26, loss = 0.73786826\n",
            "Iteration 27, loss = 0.72632581\n",
            "Iteration 28, loss = 0.71511871\n",
            "Iteration 29, loss = 0.70424319\n",
            "Iteration 30, loss = 0.69368183\n",
            "Iteration 31, loss = 0.68342580\n",
            "Iteration 32, loss = 0.67346953\n",
            "Iteration 33, loss = 0.66380608\n",
            "Iteration 34, loss = 0.65442161\n",
            "Iteration 35, loss = 0.64531278\n",
            "Iteration 36, loss = 0.63647902\n",
            "Iteration 37, loss = 0.62790342\n",
            "Iteration 38, loss = 0.61957863\n",
            "Iteration 39, loss = 0.61148527\n",
            "Iteration 40, loss = 0.60361466\n",
            "Iteration 41, loss = 0.59597076\n",
            "Iteration 42, loss = 0.58855743\n",
            "Iteration 43, loss = 0.58136358\n",
            "Iteration 44, loss = 0.57437150\n",
            "Iteration 45, loss = 0.56758016\n",
            "Iteration 46, loss = 0.56097952\n",
            "Iteration 47, loss = 0.55455113\n",
            "Iteration 48, loss = 0.54829175\n",
            "Iteration 49, loss = 0.54219766\n",
            "Iteration 50, loss = 0.53626159\n",
            "Iteration 51, loss = 0.53048210\n",
            "Iteration 52, loss = 0.52484566\n",
            "Iteration 53, loss = 0.51933866\n",
            "Iteration 54, loss = 0.51394846\n",
            "Iteration 55, loss = 0.50869354\n",
            "Iteration 56, loss = 0.50356710\n",
            "Iteration 57, loss = 0.49857160\n",
            "Iteration 58, loss = 0.49370008\n",
            "Iteration 59, loss = 0.48894487\n",
            "Iteration 60, loss = 0.48429908\n",
            "Iteration 61, loss = 0.47974886\n",
            "Iteration 62, loss = 0.47529331\n",
            "Iteration 63, loss = 0.47093384\n",
            "Iteration 64, loss = 0.46667067\n",
            "Iteration 65, loss = 0.46249978\n",
            "Iteration 66, loss = 0.45841914\n",
            "Iteration 67, loss = 0.45442291\n",
            "Iteration 68, loss = 0.45050855\n",
            "Iteration 69, loss = 0.44667746\n",
            "Iteration 70, loss = 0.44292801\n",
            "Iteration 71, loss = 0.43926405\n",
            "Iteration 72, loss = 0.43567120\n",
            "Iteration 73, loss = 0.43215951\n",
            "Iteration 74, loss = 0.42871948\n",
            "Iteration 75, loss = 0.42534205\n",
            "Iteration 76, loss = 0.42201917\n",
            "Iteration 77, loss = 0.41875279\n",
            "Iteration 78, loss = 0.41554234\n",
            "Iteration 79, loss = 0.41238463\n",
            "Iteration 80, loss = 0.40928512\n",
            "Iteration 81, loss = 0.40623752\n",
            "Iteration 82, loss = 0.40323632\n",
            "Iteration 83, loss = 0.40027444\n",
            "Iteration 84, loss = 0.39735268\n",
            "Iteration 85, loss = 0.39447038\n",
            "Iteration 86, loss = 0.39162936\n",
            "Iteration 87, loss = 0.38882549\n",
            "Iteration 88, loss = 0.38605547\n",
            "Iteration 89, loss = 0.38331954\n",
            "Iteration 90, loss = 0.38062197\n",
            "Iteration 91, loss = 0.37796519\n",
            "Iteration 92, loss = 0.37534717\n",
            "Iteration 93, loss = 0.37276083\n",
            "Iteration 94, loss = 0.37020149\n",
            "Iteration 95, loss = 0.36766960\n",
            "Iteration 96, loss = 0.36516474\n",
            "Iteration 97, loss = 0.36267665\n",
            "Iteration 98, loss = 0.36021595\n",
            "Iteration 99, loss = 0.35777846\n",
            "Iteration 100, loss = 0.35536449\n",
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "[1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Precisión del modelo: 0.9333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 1. Carga de las librerias\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Carga del conjunto de datos de Iris\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "#Exportando los datos\n",
        "\n",
        "type(iris)\n",
        "iris.keys()\n",
        "iris['data']\n",
        "iris['target']\n",
        "iris['target_names']\n",
        "iris['feature_names']\n",
        "\n",
        "# 3. Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Escalar las características para un mejor rendimiento del modelo\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Crear una instancia de MLPClassifier\n",
        "\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(100), activation='relu',solver='adam',\n",
        "                    max_iter=100, random_state=42,verbose=True)\n",
        "\n",
        "# 6. Entrenar el modelo\n",
        "\n",
        "mlp_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 7. Realizar predicciones en el conjunto de prueba\n",
        "\n",
        "y_pred = mlp_clf.predict(X_test_scaled)\n",
        "print(y_test)\n",
        "print(y_pred)\n",
        "\n",
        "# 8. Calcular la precisión del modelo\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precisión del modelo:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementación de una red neuronal para regresión utilizando MLPRegressor de Scikit-learn"
      ],
      "metadata": {
        "id": "3wAJ_uStVW0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Carga de las librerias\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 2. Carga el conjunto de datos de Boston Housing\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Explorando los datos\n",
        "type(housing)\n",
        "housing.keys()\n",
        "housing['data']\n",
        "housing['target']\n",
        "housing['target_names']\n",
        "housing['DESCR']\n",
        "housing['feature_names']\n",
        "\n",
        "# 3. Dividimos los datos en conjuntos de entrenamiento y prueba.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# 4. Escalamos las características utilizando StandardScaler para\n",
        "# asegurarnos de que todas tengan la misma escala.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Creamos una instancia de MLPRegressor con una capa oculta de\n",
        "# 100 neuronas, función de activación ReLU, algoritmo de\n",
        "# optimización 'adam' y un máximo de 500 iteraciones.\n",
        "\n",
        "mlp_reg = MLPRegressor(hidden_layer_sizes=(10,), activation='relu',\n",
        "                       solver='adam', max_iter=500, random_state=42, verbose=True)\n",
        "\n",
        "# 6. Entrenar el modelo utilizando los datos de entrenamiento escalados\n",
        "mlp_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 7. Realizar predicciones en el conjunto de prueba escalado\n",
        "y_pred = mlp_reg.predict(X_test_scaled)\n",
        "\n",
        "# 8. Calcular el error cuadrático medio (MSE) del modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio (MSE):\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_UwSkJ-bExz",
        "outputId": "11a0c2bc-5214-49d6-c57e-eb323989842f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 5.11933083\n",
            "Iteration 2, loss = 3.18784827\n",
            "Iteration 3, loss = 1.91568590\n",
            "Iteration 4, loss = 1.19232571\n",
            "Iteration 5, loss = 0.83384928\n",
            "Iteration 6, loss = 0.66382560\n",
            "Iteration 7, loss = 0.57342515\n",
            "Iteration 8, loss = 0.51641610\n",
            "Iteration 9, loss = 0.47530194\n",
            "Iteration 10, loss = 0.44222520\n",
            "Iteration 11, loss = 0.41530987\n",
            "Iteration 12, loss = 0.39154693\n",
            "Iteration 13, loss = 0.37062875\n",
            "Iteration 14, loss = 0.35197691\n",
            "Iteration 15, loss = 0.33527365\n",
            "Iteration 16, loss = 0.31992495\n",
            "Iteration 17, loss = 0.30569847\n",
            "Iteration 18, loss = 0.29302713\n",
            "Iteration 19, loss = 0.28137510\n",
            "Iteration 20, loss = 0.27090351\n",
            "Iteration 21, loss = 0.26179780\n",
            "Iteration 22, loss = 0.25414937\n",
            "Iteration 23, loss = 0.24738752\n",
            "Iteration 24, loss = 0.24175684\n",
            "Iteration 25, loss = 0.23714435\n",
            "Iteration 26, loss = 0.23345995\n",
            "Iteration 27, loss = 0.23034794\n",
            "Iteration 28, loss = 0.22782150\n",
            "Iteration 29, loss = 0.22566489\n",
            "Iteration 30, loss = 0.22409967\n",
            "Iteration 31, loss = 0.22271173\n",
            "Iteration 32, loss = 0.22167836\n",
            "Iteration 33, loss = 0.22047442\n",
            "Iteration 34, loss = 0.21975502\n",
            "Iteration 35, loss = 0.21857208\n",
            "Iteration 36, loss = 0.21762340\n",
            "Iteration 37, loss = 0.21686066\n",
            "Iteration 38, loss = 0.21605502\n",
            "Iteration 39, loss = 0.21536170\n",
            "Iteration 40, loss = 0.21459137\n",
            "Iteration 41, loss = 0.21402631\n",
            "Iteration 42, loss = 0.21341281\n",
            "Iteration 43, loss = 0.21273155\n",
            "Iteration 44, loss = 0.21211928\n",
            "Iteration 45, loss = 0.21152974\n",
            "Iteration 46, loss = 0.21136358\n",
            "Iteration 47, loss = 0.21071988\n",
            "Iteration 48, loss = 0.21015614\n",
            "Iteration 49, loss = 0.20952912\n",
            "Iteration 50, loss = 0.20916838\n",
            "Iteration 51, loss = 0.20868210\n",
            "Iteration 52, loss = 0.20831257\n",
            "Iteration 53, loss = 0.20806354\n",
            "Iteration 54, loss = 0.20771823\n",
            "Iteration 55, loss = 0.20743089\n",
            "Iteration 56, loss = 0.20691611\n",
            "Iteration 57, loss = 0.20662210\n",
            "Iteration 58, loss = 0.20650795\n",
            "Iteration 59, loss = 0.20588719\n",
            "Iteration 60, loss = 0.20556654\n",
            "Iteration 61, loss = 0.20524801\n",
            "Iteration 62, loss = 0.20497526\n",
            "Iteration 63, loss = 0.20447939\n",
            "Iteration 64, loss = 0.20421767\n",
            "Iteration 65, loss = 0.20390132\n",
            "Iteration 66, loss = 0.20372868\n",
            "Iteration 67, loss = 0.20316553\n",
            "Iteration 68, loss = 0.20279280\n",
            "Iteration 69, loss = 0.20247354\n",
            "Iteration 70, loss = 0.20204458\n",
            "Iteration 71, loss = 0.20181043\n",
            "Iteration 72, loss = 0.20156168\n",
            "Iteration 73, loss = 0.20149167\n",
            "Iteration 74, loss = 0.20113628\n",
            "Iteration 75, loss = 0.20055187\n",
            "Iteration 76, loss = 0.20012264\n",
            "Iteration 77, loss = 0.19960515\n",
            "Iteration 78, loss = 0.19928422\n",
            "Iteration 79, loss = 0.19873798\n",
            "Iteration 80, loss = 0.19859383\n",
            "Iteration 81, loss = 0.19822745\n",
            "Iteration 82, loss = 0.19784549\n",
            "Iteration 83, loss = 0.19745515\n",
            "Iteration 84, loss = 0.19697734\n",
            "Iteration 85, loss = 0.19672791\n",
            "Iteration 86, loss = 0.19622307\n",
            "Iteration 87, loss = 0.19590699\n",
            "Iteration 88, loss = 0.19530398\n",
            "Iteration 89, loss = 0.19484472\n",
            "Iteration 90, loss = 0.19455767\n",
            "Iteration 91, loss = 0.19382560\n",
            "Iteration 92, loss = 0.19384405\n",
            "Iteration 93, loss = 0.19305022\n",
            "Iteration 94, loss = 0.19271097\n",
            "Iteration 95, loss = 0.19220966\n",
            "Iteration 96, loss = 0.19147751\n",
            "Iteration 97, loss = 0.19099382\n",
            "Iteration 98, loss = 0.19039776\n",
            "Iteration 99, loss = 0.19025237\n",
            "Iteration 100, loss = 0.18957883\n",
            "Iteration 101, loss = 0.18882938\n",
            "Iteration 102, loss = 0.18828588\n",
            "Iteration 103, loss = 0.18798281\n",
            "Iteration 104, loss = 0.18767482\n",
            "Iteration 105, loss = 0.18681687\n",
            "Iteration 106, loss = 0.18626829\n",
            "Iteration 107, loss = 0.18592954\n",
            "Iteration 108, loss = 0.18522314\n",
            "Iteration 109, loss = 0.18472908\n",
            "Iteration 110, loss = 0.18406255\n",
            "Iteration 111, loss = 0.18322541\n",
            "Iteration 112, loss = 0.18278847\n",
            "Iteration 113, loss = 0.18254734\n",
            "Iteration 114, loss = 0.18204760\n",
            "Iteration 115, loss = 0.18131156\n",
            "Iteration 116, loss = 0.18083985\n",
            "Iteration 117, loss = 0.18043861\n",
            "Iteration 118, loss = 0.17950755\n",
            "Iteration 119, loss = 0.17917773\n",
            "Iteration 120, loss = 0.17849036\n",
            "Iteration 121, loss = 0.17809884\n",
            "Iteration 122, loss = 0.17799730\n",
            "Iteration 123, loss = 0.17711941\n",
            "Iteration 124, loss = 0.17660859\n",
            "Iteration 125, loss = 0.17648458\n",
            "Iteration 126, loss = 0.17582317\n",
            "Iteration 127, loss = 0.17534377\n",
            "Iteration 128, loss = 0.17533571\n",
            "Iteration 129, loss = 0.17490297\n",
            "Iteration 130, loss = 0.17420696\n",
            "Iteration 131, loss = 0.17385435\n",
            "Iteration 132, loss = 0.17365971\n",
            "Iteration 133, loss = 0.17322615\n",
            "Iteration 134, loss = 0.17354934\n",
            "Iteration 135, loss = 0.17260033\n",
            "Iteration 136, loss = 0.17207497\n",
            "Iteration 137, loss = 0.17163495\n",
            "Iteration 138, loss = 0.17147383\n",
            "Iteration 139, loss = 0.17126699\n",
            "Iteration 140, loss = 0.17068356\n",
            "Iteration 141, loss = 0.17021440\n",
            "Iteration 142, loss = 0.17016601\n",
            "Iteration 143, loss = 0.16988828\n",
            "Iteration 144, loss = 0.16971219\n",
            "Iteration 145, loss = 0.16929570\n",
            "Iteration 146, loss = 0.16903044\n",
            "Iteration 147, loss = 0.16895280\n",
            "Iteration 148, loss = 0.16846334\n",
            "Iteration 149, loss = 0.16831929\n",
            "Iteration 150, loss = 0.16822020\n",
            "Iteration 151, loss = 0.16806698\n",
            "Iteration 152, loss = 0.16872686\n",
            "Iteration 153, loss = 0.16803569\n",
            "Iteration 154, loss = 0.16730673\n",
            "Iteration 155, loss = 0.16704433\n",
            "Iteration 156, loss = 0.16671321\n",
            "Iteration 157, loss = 0.16688807\n",
            "Iteration 158, loss = 0.16678905\n",
            "Iteration 159, loss = 0.16643933\n",
            "Iteration 160, loss = 0.16624821\n",
            "Iteration 161, loss = 0.16597303\n",
            "Iteration 162, loss = 0.16586510\n",
            "Iteration 163, loss = 0.16584143\n",
            "Iteration 164, loss = 0.16552461\n",
            "Iteration 165, loss = 0.16611297\n",
            "Iteration 166, loss = 0.16532653\n",
            "Iteration 167, loss = 0.16554627\n",
            "Iteration 168, loss = 0.16487902\n",
            "Iteration 169, loss = 0.16492273\n",
            "Iteration 170, loss = 0.16488665\n",
            "Iteration 171, loss = 0.16476832\n",
            "Iteration 172, loss = 0.16469876\n",
            "Iteration 173, loss = 0.16471345\n",
            "Iteration 174, loss = 0.16503156\n",
            "Iteration 175, loss = 0.16429462\n",
            "Iteration 176, loss = 0.16433206\n",
            "Iteration 177, loss = 0.16425873\n",
            "Iteration 178, loss = 0.16437976\n",
            "Iteration 179, loss = 0.16405295\n",
            "Iteration 180, loss = 0.16411211\n",
            "Iteration 181, loss = 0.16383892\n",
            "Iteration 182, loss = 0.16383052\n",
            "Iteration 183, loss = 0.16361265\n",
            "Iteration 184, loss = 0.16343761\n",
            "Iteration 185, loss = 0.16370496\n",
            "Iteration 186, loss = 0.16539138\n",
            "Iteration 187, loss = 0.16365031\n",
            "Iteration 188, loss = 0.16341338\n",
            "Iteration 189, loss = 0.16319709\n",
            "Iteration 190, loss = 0.16337377\n",
            "Iteration 191, loss = 0.16344390\n",
            "Iteration 192, loss = 0.16356990\n",
            "Iteration 193, loss = 0.16332121\n",
            "Iteration 194, loss = 0.16331489\n",
            "Iteration 195, loss = 0.16344922\n",
            "Iteration 196, loss = 0.16277483\n",
            "Iteration 197, loss = 0.16281194\n",
            "Iteration 198, loss = 0.16267489\n",
            "Iteration 199, loss = 0.16276417\n",
            "Iteration 200, loss = 0.16280330\n",
            "Iteration 201, loss = 0.16273294\n",
            "Iteration 202, loss = 0.16275271\n",
            "Iteration 203, loss = 0.16304014\n",
            "Iteration 204, loss = 0.16282974\n",
            "Iteration 205, loss = 0.16258176\n",
            "Iteration 206, loss = 0.16250006\n",
            "Iteration 207, loss = 0.16246767\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Error cuadrático medio (MSE): 0.3441578388254657\n"
          ]
        }
      ]
    }
  ]
}